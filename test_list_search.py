#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ›¿ä»£æ–¹æ¡ˆï¼šé€šè¿‡åˆ—è¡¨é¡µå®ç°æœç´¢åŠŸèƒ½
é¿å…æœç´¢å¹³å°çš„åçˆ¬é™åˆ¶ï¼Œç›´æ¥çˆ¬å–åˆ†ç±»åˆ—è¡¨å¹¶è¿‡æ»¤
"""
import sys
from pathlib import Path
from datetime import datetime, timedelta

sys.path.insert(0, str(Path(__file__).parent))

from scraper.fetcher import PlaywrightFetcher
from scraper.ccgp_parser import CCGPAnnouncementParser
from bs4 import BeautifulSoup


def search_via_list_pages(
    keyword: str = "æ™ºèƒ½",
    category: str = "engineering",  # engineering, goods, services
    days: int = 1,  # æœ€è¿‘å‡ å¤©
    max_results: int = 20,
):
    """
    é€šè¿‡åˆ—è¡¨é¡µæœç´¢å…¬å‘Š

    Args:
        keyword: æœç´¢å…³é”®è¯ï¼ˆå¦‚"æ™ºèƒ½"ï¼‰
        category: å“ç›®ç±»åˆ«
        days: æœç´¢æœ€è¿‘å‡ å¤©çš„å…¬å‘Š
        max_results: æœ€å¤šè¿”å›ç»“æœæ•°
    """
    print("=" * 70)
    print("æ”¿åºœé‡‡è´­ç½‘åˆ—è¡¨é¡µæœç´¢")
    print("=" * 70)

    # åˆ†ç±»URLæ˜ å°„
    category_urls = {
        'engineering': 'https://www.ccgp.gov.cn/cggg/gcgg/index.htm',  # å·¥ç¨‹ç±»
        'goods': 'https://www.ccgp.gov.cn/cggg/hwggg/index.htm',     # è´§ç‰©ç±»
        'services': 'https://www.ccgp.gov.cn/cggg/fwgpg/index.htm',   # æœåŠ¡ç±»
        'central': 'https://www.ccgp.gov.cn/cggg/zygg/index.htm',      # ä¸­å¤®å…¬å‘Š
        'winning': 'https://www.ccgp.gov.cn/cggg/zybg/index.htm',      # ä¸­æ ‡å…¬å‘Š
        # å·¥ç¨‹ç±»ä¸­æ ‡å…¬å‘Šï¼ˆæœ€å¯èƒ½æœ‰æœºæˆ¿é¡¹ç›®ï¼‰
        'engineering_winning': 'https://www.ccgp.gov.cn/cggg/gcgg/zbgg/index.htm',
    }

    target_url = category_urls.get(category)
    if not target_url:
        print(f"âŒ ä¸æ”¯æŒçš„ç±»åˆ«: {category}")
        return

    print(f"\nğŸ” æœç´¢æ¡ä»¶:")
    print(f"  å…³é”®è¯: {keyword}")
    print(f"  å“ç›®: {category}")
    print(f"  æ—¶é—´: æœ€è¿‘ {days} å¤©")
    print(f"  æ¥æº: {target_url}")

    # åˆ›å»ºçˆ¬è™«
    fetcher = PlaywrightFetcher()

    try:
        fetcher.start()

        # è®¡ç®—ç›®æ ‡æ—¥æœŸ
        target_date = datetime.now() - timedelta(days=days)
        date_str = target_date.strftime('%Yå¹´%mæœˆ%dæ—¥')

        print(f"\næ­£åœ¨çˆ¬å–åˆ—è¡¨é¡µ...")

        # è·å–åˆ—è¡¨é¡µ
        html = fetcher.get_page(target_url)
        if not html:
            print("âŒ è·å–åˆ—è¡¨é¡µå¤±è´¥")
            return

        # è§£æåˆ—è¡¨é¡µ
        soup = BeautifulSoup(html, 'lxml')

        # æŸ¥æ‰¾æ‰€æœ‰å…¬å‘Šé“¾æ¥
        all_links = []
        for link in soup.find_all('a'):
            href = link.get('href', '')
            title = link.get_text(strip=True)

            # è¿‡æ»¤æœ‰æ•ˆçš„å…¬å‘Šé“¾æ¥
            if (
                href
                and 'htm' in href
                and len(title) > 10
                and title not in [item.get('title') for item in all_links]
            ):
                all_links.append({
                    'title': title,
                    'href': href,
                })

        print(f"âœ… æ‰¾åˆ° {len(all_links)} ä¸ªå…¬å‘Šé“¾æ¥")

        # ===== å…³é”®è¯è¿‡æ»¤ =====
        print(f"\næ­£åœ¨è¿‡æ»¤åŒ…å«å…³é”®è¯ '{keyword}' çš„å…¬å‘Š...")

        filtered = []
        for item in all_links:
            if keyword in item['title']:
                # æ„å»ºå®Œæ•´URL
                href = item['href']

                # ä½¿ç”¨urllib.parse.urljoinæ­£ç¡®å¤„ç†ç›¸å¯¹è·¯å¾„
                from urllib.parse import urljoin
                url = urljoin('https://www.ccgp.gov.cn/cggg/zygg/', href)

                filtered.append({
                    'title': item['title'],
                    'url': url,
                    'source': category,
                })

        print(f"âœ… è¿‡æ»¤åå‰©ä½™ {len(filtered)} æ¡å…¬å‘Š")

        if not filtered:
            print(f"\nâš ï¸ æ²¡æœ‰æ‰¾åˆ°åŒ…å«'{keyword}'çš„å…¬å‘Š")
            return

        # æ˜¾ç¤ºæœç´¢ç»“æœ
        print(f"\næœç´¢ç»“æœ (å‰10æ¡):")
        print("-" * 70)

        for i, item in enumerate(filtered[:10], 1):
            print(f"{i}. {item['title'][:60]}")
            print(f"   {item['url']}")

        if len(filtered) > 10:
            print(f"\n... è¿˜æœ‰ {len(filtered) - 10} æ¡ç»“æœ")

        # ===== çˆ¬å–è¯¦æƒ…é¡µ =====
        print(f"\næ­£åœ¨çˆ¬å–è¯¦æƒ…é¡µ (å‰{min(max_results, len(filtered))}æ¡)...")

        parser = CCGPAnnouncementParser()
        detailed_results = []

        for i, item in enumerate(filtered[:max_results], 1):
            print(f"\n[{i}/{len(filtered[:max_results])}] {item['title'][:40]}")

            try:
                # è·å–è¯¦æƒ…é¡µ
                detail_html = fetcher.get_page(item['url'])
                if not detail_html:
                    print("   âŒ è·å–è¯¦æƒ…é¡µå¤±è´¥")
                    continue

                # è§£æè¯¦æƒ…é¡µ
                parsed = parser.parse(detail_html, item['url'])
                formatted = parser.format_for_storage(parsed)

                detailed_results.append(formatted)

                # æ˜¾ç¤ºå…³é”®ä¿¡æ¯
                print(f"   âœ… é‡‡è´­äºº: {formatted.get('buyer_name', '')[:30]}")
                print(f"   ğŸ† ä¸­æ ‡äºº: {formatted.get('supplier', '')[:30]}")
                print(f"   ğŸ’° é‡‘é¢: {formatted.get('bid_amount', '')[:30]}")

            except Exception as e:
                print(f"   âŒ å¤±è´¥: {e}")

        # ===== ä¿å­˜ç»“æœ =====
        print(f"\nä¿å­˜ç»“æœ...")

        import json
        output_file = Path("data/list_search_results.json")
        output_file.parent.mkdir(exist_ok=True)

        save_data = {
            'search_params': {
                'keyword': keyword,
                'category': category,
                'days': days,
                'search_time': datetime.now().isoformat(),
            },
            'summary': {
                'total_found': len(filtered),
                'detailed_crawled': len(detailed_results),
            },
            'filtered_results': filtered,
            'detailed_results': detailed_results,
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)

        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

        # ===== ç»Ÿè®¡ä¿¡æ¯ =====
        print(f"\n" + "=" * 70)
        print("ç»Ÿè®¡ä¿¡æ¯:")
        print("=" * 70)
        print(f"æ‰¾åˆ°ç›¸å…³å…¬å‘Š: {len(filtered)} æ¡")
        print(f"æˆåŠŸè§£æè¯¦æƒ…: {len(detailed_results)} æ¡")

        # ç»Ÿè®¡ä¸­æ ‡äºº
        suppliers = {}
        for r in detailed_results:
            supplier = r.get('supplier', '')
            if supplier:
                suppliers[supplier] = suppliers.get(supplier, 0) + 1

        if suppliers:
            print(f"\nä¸­æ ‡ä¼ä¸š (æŒ‰é¡¹ç›®æ•°æ’åº):")
            for supplier, count in sorted(suppliers.items(), key=lambda x: -x[1])[:10]:
                print(f"  - {supplier}: {count} ä¸ªé¡¹ç›®")

    finally:
        fetcher.stop()

    print("\n" + "=" * 70)
    print("âœ… æœç´¢å®Œæˆï¼")
    print("=" * 70)


if __name__ == '__main__':
    # æœç´¢"æœºæˆ¿"ç›¸å…³çš„å…¬å‘Š
    # å°è¯•å·¥ç¨‹ç±»ä¸­æ ‡å…¬å‘Šï¼ˆæœ€å¯èƒ½æœ‰æœºæˆ¿é¡¹ç›®ï¼‰
    print("å°è¯•å¤šä¸ªåˆ†ç±»æœç´¢...\n")

    # å…ˆå°è¯•å·¥ç¨‹ç±»ä¸­æ ‡å…¬å‘Š
    print("=" * 70)
    print("ã€å°è¯•1ã€‘å·¥ç¨‹ç±»ä¸­æ ‡å…¬å‘Š")
    print("=" * 70)
    search_via_list_pages(
        keyword="æœºæˆ¿",
        category="engineering_winning",  # å·¥ç¨‹ç±»ä¸­æ ‡å…¬å‘Š
        days=90,               # æœ€è¿‘90å¤©
        max_results=10,
    )
