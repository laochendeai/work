#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•å¢å¼ºç‰ˆæœç´¢çˆ¬è™« - æœç´¢"æœºæˆ¿"
ä½¿ç”¨å¤šç§ç­–ç•¥é™ä½åçˆ¬é£é™©
"""
import sys
from pathlib import Path
import json
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent))

from scraper.fetcher import PlaywrightFetcher
from scraper.ccgp_searcher_v2 import CCGPSearcherEnhanced
from scraper.ccgp_parser import CCGPAnnouncementParser


def test_enhanced_search_jifang():
    """
    æµ‹è¯•å¢å¼ºç‰ˆæœç´¢ - æœå…¨æ–‡"æœºæˆ¿"

    ä½¿ç”¨ç­–ç•¥ï¼š
    1. éšæœºUser-Agent
    2. æ¨¡æ‹Ÿäººå·¥æ“ä½œï¼ˆé€å­—è¾“å…¥ã€éšæœºå»¶è¿Ÿï¼‰
    3. å…ˆè®¿é—®é¦–é¡µå»ºç«‹ä¼šè¯
    4. æ¯æ­¥æ“ä½œéƒ½æœ‰éšæœºå»¶è¿Ÿ
    """

    print("=" * 70)
    print("å…¬å…±èµ„æºäº¤æ˜“ç½‘æœç´¢ - å¢å¼ºç‰ˆæµ‹è¯•")
    print("æœç´¢å…³é”®è¯ï¼šæœºæˆ¿")
    print("=" * 70)

    # æœç´¢å‚æ•°
    keyword = "æœºæˆ¿"
    search_type = "fulltext"  # æœå…¨æ–‡
    category = "engineering"   # å·¥ç¨‹ç±»
    time_range = "1month"      # è¿‘1æœˆ
    max_pages = 2             # æµ‹è¯•åªçˆ¬2é¡µ

    print(f"\nğŸ” æœç´¢å‚æ•°:")
    print(f"  å…³é”®è¯: {keyword}")
    print(f"  æœç´¢ç±»å‹: {search_type}")
    print(f"  å“ç›®: {category}")
    print(f"  æ—¶é—´: {time_range}")
    print(f"  æœ€å¤§é¡µæ•°: {max_pages}")

    print(f"\nğŸ›¡ï¸ åè§„é¿ç­–ç•¥:")
    print(f"  âœ… éšæœºUser-Agent")
    print(f"  âœ… æ¨¡æ‹Ÿäººå·¥æ“ä½œï¼ˆé€å­—è¾“å…¥ï¼‰")
    print(f"  âœ… éšæœºå»¶è¿Ÿï¼ˆ3-6ç§’ï¼‰")
    print(f"  âœ… å…ˆè®¿é—®é¦–é¡µå»ºç«‹ä¼šè¯")
    print(f"  âœ… æ¨¡æ‹Ÿäººç±»æµè§ˆè¡Œä¸º")

    # åˆ›å»ºçˆ¬è™«
    fetcher = PlaywrightFetcher()

    try:
        fetcher.start()

        # ä½¿ç”¨å¢å¼ºç‰ˆæœç´¢å™¨
        searcher = CCGPSearcherEnhanced(fetcher.page)

        print(f"\nå¼€å§‹æœç´¢ï¼ˆè¿™å¯èƒ½éœ€è¦1-2åˆ†é’Ÿï¼‰...")

        # æ‰§è¡Œæœç´¢
        results = searcher.search(
            keyword=keyword,
            search_type=search_type,
            category=category,
            time_range=time_range,
            max_pages=max_pages,
        )

        print(f"\n{'=' * 70}")
        print(f"æœç´¢ç»“æœ: {len(results)} æ¡")
        print(f"{'=' * 70}")

        if not results:
            print("\nâš ï¸ æœªæ‰¾åˆ°æœç´¢ç»“æœ")
            print("\nå¯èƒ½çš„åŸå› :")
            print("  1. åçˆ¬é™åˆ¶ - å»ºè®®ç¨åå†è¯•ï¼ˆç­‰å¾…10-30åˆ†é’Ÿï¼‰")
            print("  2. æœç´¢æ¡ä»¶è¿‡ä¸¥ - å¯ä»¥å°è¯•æ‰©å¤§æ—¶é—´èŒƒå›´")
            print("  3. é€‰æ‹©å™¨å¤±æ•ˆ - ç½‘ç«™å¯èƒ½å·²æ›´æ–°ç»“æ„")
            print("  4. ä»Šæ—¥ç¡®å®æ²¡æœ‰ç›¸å…³å…¬å‘Š - å¯ä»¥å°è¯•æ—¶é—´èŒƒå›´æ”¹ä¸º'1month'")
            print("\nğŸ’¡ å»ºè®®å°è¯•:")
            print("  - ä½¿ç”¨åˆ—è¡¨é¡µçˆ¬å–æ–¹å¼ï¼ˆæ›´ç¨³å®šï¼‰")
            print("  - æ”¹ç”¨å…¶ä»–å…³é”®è¯ï¼ˆå¦‚'æ™ºèƒ½åŒ–'ã€'å¼±ç”µ'ç­‰ï¼‰")
            print("  - æ‰©å¤§æ—¶é—´èŒƒå›´åˆ°'1month'æˆ–'3months'")
            return

        # æ˜¾ç¤ºæœç´¢ç»“æœ
        print(f"\næœç´¢ç»“æœ:")
        print("-" * 70)

        for i, result in enumerate(results, 1):
            title = result.get('title', 'æœªçŸ¥æ ‡é¢˜')
            # é«˜äº®æ˜¾ç¤º"æœºæˆ¿"å…³é”®è¯
            highlighted = title.replace('æœºæˆ¿', 'â­æœºæˆ¿â­')
            print(f"{i}. {highlighted[:80]}")

        # ===== çˆ¬å–è¯¦æƒ…é¡µï¼ˆå¦‚æœæœç´¢æˆåŠŸï¼‰=====
        if results:
            print(f"\n{'=' * 70}")
            print("æ˜¯å¦è¦çˆ¬å–è¯¦æƒ…é¡µï¼Ÿ(æ¼”ç¤ºå‰2æ¡)")
            print(f"{'=' * 70}")

            # åªçˆ¬å‰2ä¸ªç»“æœä½œä¸ºæ¼”ç¤º
            crawler_results = []
            parser = CCGPAnnouncementParser()

            for i, result in enumerate(results[:2], 1):
                url = result.get('url', '')
                if not url:
                    continue

                print(f"\n[{i}/2] {result.get('title', '')[:50]}")
                print(f"     URL: {url}")

                try:
                    # è·å–è¯¦æƒ…é¡µ - å¢åŠ å»¶è¿Ÿ
                    import time
                    time.sleep(3)

                    detail_html = fetcher.get_page(url)
                    if not detail_html:
                        print("     âŒ è·å–è¯¦æƒ…é¡µå¤±è´¥")
                        continue

                    # è§£æè¯¦æƒ…é¡µ
                    parsed = parser.parse(detail_html, url)
                    formatted = parser.format_for_storage(parsed)

                    crawler_results.append(formatted)

                    # æ˜¾ç¤ºå…³é”®ä¿¡æ¯
                    print(f"     âœ… é¡¹ç›®: {formatted.get('project_name', '')[:40]}")
                    print(f"     ğŸ“ é‡‡è´­äºº: {formatted.get('buyer_name', '')[:30]}")
                    print(f"     ğŸ† ä¸­æ ‡äºº: {formatted.get('supplier', '')[:30]}")
                    print(f"     ğŸ’° é‡‘é¢: {formatted.get('bid_amount', '')[:30]}")

                except Exception as e:
                    print(f"     âŒ å¤±è´¥: {e}")

        # ===== ä¿å­˜ç»“æœ =====
        print(f"\nä¿å­˜ç»“æœ...")

        output_file = Path("data/search_jifang_results.json")
        output_file.parent.mkdir(exist_ok=True)

        save_data = {
            'search_params': {
                'keyword': keyword,
                'search_type': search_type,
                'category': category,
                'time_range': time_range,
                'search_time': datetime.now().isoformat(),
            },
            'summary': {
                'total_results': len(results),
                'detailed_crawled': len(crawler_results) if results else 0,
            },
            'search_results': results,
            'detailed_results': crawler_results if results else [],
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)

        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

        # ===== ç»Ÿè®¡ä¿¡æ¯ =====
        print(f"\n{'=' * 70}")
        print("ç»Ÿè®¡ä¿¡æ¯:")
        print(f"{'=' * 70}")
        print(f"æœç´¢ç»“æœæ€»æ•°: {len(results)}")

        if results and 'crawler_results' in locals():
            print(f"æˆåŠŸè§£æè¯¦æƒ…: {len(crawler_results)}")

            # ç»Ÿè®¡åŒ…å«"æœºæˆ¿"çš„é¡¹ç›®
            jifang_projects = [
                r for r in crawler_results
                if 'æœºæˆ¿' in r.get('project_name', '') or 'æœºæˆ¿' in r.get('title', '')
            ]

            if jifang_projects:
                print(f"\nåŒ…å«'æœºæˆ¿'çš„é¡¹ç›®è¯¦æƒ…:")
                for r in jifang_projects:
                    print(f"  - {r.get('project_name', '')[:60]}")
                    print(f"    é‡‡è´­äºº: {r.get('buyer_name', '')}")
                    print(f"    ä¸­æ ‡äºº: {r.get('supplier', '')}")

    finally:
        fetcher.stop()

    print(f"\n{'=' * 70}")
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print(f"{'=' * 70}")

    # ===== é‡è¦æé†’ =====
    print(f"\nâš ï¸  é‡è¦æé†’:")
    print(f"  1. å¦‚æœè§¦å‘åçˆ¬é™åˆ¶ï¼Œè¯·ç­‰å¾…10-30åˆ†é’Ÿåå†è¯•")
    print(f"  2. å»ºè®®æ¯å¤©æœç´¢æ¬¡æ•°ä¸è¶…è¿‡3-5æ¬¡")
    print(f"  3. æ¯æ¬¡æœç´¢é—´éš”è‡³å°‘30åˆ†é’Ÿ")
    print(f"  4. é¿å¼€å·¥ä½œæ—¶é—´é«˜å³°ï¼ˆ9:00-17:00ï¼‰")
    print(f"  5. å¦‚éœ€å¤§é‡æ•°æ®ï¼Œä¼˜å…ˆä½¿ç”¨åˆ—è¡¨é¡µçˆ¬å–æ–¹å¼")

    print(f"\nğŸ’¡ æ›¿ä»£æ–¹æ¡ˆ:")
    print(f"  å¦‚æœæœç´¢å¹³å°å—é™ï¼Œå¯ä»¥ä½¿ç”¨åˆ—è¡¨é¡µçˆ¬å– + æœ¬åœ°è¿‡æ»¤")
    print(f"  å‚è€ƒ: test_list_search.py")


if __name__ == '__main__':
    test_enhanced_search_jifang()
