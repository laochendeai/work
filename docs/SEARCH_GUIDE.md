# å…¬å…±èµ„æºäº¤æ˜“ç½‘æœç´¢åŠŸèƒ½è¯´æ˜

## âš ï¸ é‡è¦æç¤º

ä¸­å›½å…¬å…±èµ„æºäº¤æ˜“ç½‘çš„æœç´¢å¹³å°å…·æœ‰**åçˆ¬ä¿æŠ¤æœºåˆ¶**ï¼Œç›´æ¥æ¨¡æ‹Ÿé¡µé¢æ“ä½œå¯èƒ½ä¼šè¢«é™åˆ¶è®¿é—®ã€‚

### åçˆ¬æœºåˆ¶
- âœ… è®¿é—®é¢‘ç‡é™åˆ¶ï¼ˆé¢‘ç¹è®¿é—®ä¼šè¢«å°ç¦ï¼‰
- âœ… IPåœ°å€è¿½è¸ª
- âœ… å¯èƒ½è§¦å‘éªŒè¯ç 

---

## âœ… æ¨èæ–¹æ¡ˆ

### æ–¹æ¡ˆ1ï¼šç›´æ¥çˆ¬å–åˆ†ç±»åˆ—è¡¨é¡µ + å…³é”®è¯è¿‡æ»¤

è¿™æ˜¯**æœ€ç®€å•ã€æœ€ç¨³å®š**çš„æ–¹æ³•ï¼š

```python
from scraper.fetcher import PlaywrightFetcher
from bs4 import BeautifulSoup

# 1. åˆ›å»ºçˆ¬è™«
fetcher = PlaywrightFetcher()
fetcher.start()

# 2. çˆ¬å–ä¸­å¤®å…¬å‘Šåˆ—è¡¨é¡µ
url = "https://www.ccgp.gov.cn/cggg/zygg/index.htm"
html = fetcher.get_page(url)

# 3. è§£æåˆ—è¡¨é¡µ
soup = BeautifulSoup(html, 'lxml')
links = soup.find_all('a')

# 4. è¿‡æ»¤åŒ…å«"æ™ºèƒ½"çš„å…¬å‘Š
keyword = "æ™ºèƒ½"
results = []

for link in links:
    title = link.get_text(strip=True)
    href = link.get('href', '')

    # åªä¿ç•™åŒ…å«å…³é”®è¯çš„å…¬å‘Š
    if keyword in title and 'htm' in href:
        results.append({
            'title': title,
            'url': f"https://www.ccgp.gov.cn{href}"
        })

print(f"æ‰¾åˆ° {len(results)} æ¡åŒ…å«'{keyword}'çš„å…¬å‘Š")

# 5. é€ä¸ªè®¿é—®è¯¦æƒ…é¡µ
from scraper.ccgp_parser import CCGPAnnouncementParser
parser = CCGPAnnouncementParser()

for result in results[:10]:  # æµ‹è¯•å‰10æ¡
    detail_html = fetcher.get_page(result['url'])
    parsed = parser.parse(detail_html, result['url'])
    formatted = parser.format_for_storage(parsed)

    print(f"é¡¹ç›®: {formatted['project_name']}")
    print(f"ä¸­æ ‡äºº: {formatted['supplier']}")
    print(f"é‡‘é¢: {formatted['bid_amount']}")
    print("-" * 50)

fetcher.stop()
```

### æ–¹æ¡ˆ2ï¼šçˆ¬å–å¤šä¸ªåˆ†ç±»é¡µé¢

```python
categories = {
    'ä¸­å¤®å…¬å‘Š': 'https://www.ccgp.gov.cn/cggg/zygg/index.htm',
    'è´§ç‰©ç±»': 'https://www.ccgp.gov.cn/cggg/hwggg/index.htm',
    'å·¥ç¨‹ç±»': 'https://www.ccgp.gov.cn/cggg/gcgg/index.htm',
    'æœåŠ¡ç±»': 'https://www.ccgp.gov.cn/cggg/fwgpg/index.htm',
}

keyword = "æ™ºèƒ½"
all_results = []

for category_name, url in categories.items():
    print(f"æ­£åœ¨çˆ¬å–: {category_name}")
    # çˆ¬å–å¹¶è¿‡æ»¤...
```

---

## ğŸ“Š å¯ç”¨çš„åˆ†ç±»åˆ—è¡¨é¡µ

| åˆ†ç±» | URL | è¯´æ˜ |
|------|-----|------|
| ä¸­å¤®å…¬å‘Š | /cggg/zygg/index.htm | å›½åŠ¡é™¢éƒ¨å§”å…¬å‘Š |
| è´§ç‰©ç±» | /cggg/hwggg/index.htm | è´§ç‰©é‡‡è´­å…¬å‘Š |
| å·¥ç¨‹ç±» | /cggg/gcgg/index.htm | å·¥ç¨‹é‡‡è´­å…¬å‘Š |
| æœåŠ¡ç±» | /cggg/fwgpg/index.htm | æœåŠ¡é‡‡è´­å…¬å‘Š |
| ä¸­æ ‡å…¬å‘Š | /cggg/zybg/index.htm | ä¸­æ ‡ç»“æœå…¬å‘Š |
| æˆäº¤å…¬å‘Š | /cggg/cjgg/index.htm | æˆäº¤ç»“æœå…¬å‘Š |

---

## ğŸ¯ å®é™…ä½¿ç”¨å»ºè®®

### ç­–ç•¥1ï¼šæŒ‰æ—¥æœŸçˆ¬å– + è¿‡æ»¤

```python
# çˆ¬å–ä»Šæ—¥å…¬å‘Šåï¼Œåœ¨ä»£ç ä¸­è¿‡æ»¤
from datetime import datetime, timedelta

today = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')

# è§£æåè¿‡æ»¤
filtered_results = [
    r for r in all_results
    if today in r.get('publish_date', '')
    and 'æ™ºèƒ½' in r.get('title', '')
]
```

### ç­–ç•¥2ï¼šåˆ†æ—¶çˆ¬å–

```python
import time

categories = ['zygg', 'gczb', 'zybg']
for cat in categories:
    # çˆ¬å–ä¸€ä¸ªåˆ†ç±»
    scrape_category(cat)

    # ç­‰å¾…5-10åˆ†é’Ÿå†çˆ¬ä¸‹ä¸€ä¸ª
    time.sleep(300)
```

### ç­–ç•¥3ï¼šä½¿ç”¨ä»£ç†IP

å¦‚æœéœ€è¦å¤§é‡çˆ¬å–ï¼Œå¯ä»¥ä½¿ç”¨ä»£ç†IPæ± ï¼š
```python
# åœ¨Playwrightä¸­é…ç½®ä»£ç†
context = browser.new_context(
    proxy={
        "server": "http://proxy.example.com:8080",
        "username": "user",
        "password": "pass"
    }
)
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

1. **é™ä½è¯·æ±‚é¢‘ç‡** - æ¯æ¬¡è¯·æ±‚é—´éš”3-5ç§’
2. **åˆ†æ—¶æ®µçˆ¬å–** - é¿å¼€é«˜å³°æ—¶æ®µ
3. **ä½¿ç”¨åˆ—è¡¨é¡µ** - æ¯”æœç´¢å¹³å°æ›´ç¨³å®š
4. **æœ¬åœ°è¿‡æ»¤** - çˆ¬å–ååœ¨ä»£ç ä¸­æŒ‰æ¡ä»¶ç­›é€‰
5. **éµå®ˆè§„åˆ™** - ä»…ç”¨äºåˆæ³•çš„æ•°æ®é‡‡é›†éœ€æ±‚

---

## ğŸ”§ å®Œæ•´ç¤ºä¾‹

é¡¹ç›®ä¸­çš„ `test_search.py` å±•ç¤ºäº†å®Œæ•´çš„æœç´¢æµç¨‹ï¼Œä½†ç”±äºåçˆ¬é™åˆ¶ï¼Œå»ºè®®ï¼š

1. å…ˆçˆ¬å–åˆ—è¡¨é¡µ
2. åœ¨å†…å­˜ä¸­è¿‡æ»¤å…³é”®è¯
3. é€ä¸ªè®¿é—®è¯¦æƒ…é¡µ
4. å­˜å‚¨åˆ°æ•°æ®åº“

è¿™æ ·å¯ä»¥é¿å…è§¦å‘æœç´¢å¹³å°çš„åçˆ¬æœºåˆ¶ï¼ŒåŒæ—¶å®ç°ç›¸åŒçš„åŠŸèƒ½ã€‚
