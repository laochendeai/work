#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•å…¬å…±èµ„æºäº¤æ˜“ç½‘æ™ºèƒ½æœç´¢
å®Œå…¨æ¨¡æ‹Ÿäººå·¥æ“ä½œï¼šæœç´¢"æ™ºèƒ½"ï¼Œå·¥ç¨‹ç±»ï¼Œä»Šæ—¥
"""
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent))

from scraper.fetcher import PlaywrightFetcher
from scraper.ccgp_searcher import CCGPSearcher
from scraper.ccgp_parser import CCGPAnnouncementParser


def test_intelligent_search():
    """
    æµ‹è¯•æ™ºèƒ½æœç´¢åŠŸèƒ½

    æ¨¡æ‹Ÿæ“ä½œï¼š
    1. æ‰“å¼€æœç´¢å¹³å°
    2. è®¾ç½®å“ç›®ä¸º"å·¥ç¨‹ç±»"
    3. è®¾ç½®æ—¶é—´ä¸º"ä»Šæ—¥"
    4. è¾“å…¥å…³é”®è¯"æ™ºèƒ½"
    5. ç‚¹å‡»"æœå…¨æ–‡"
    6. çˆ¬å–æœç´¢ç»“æœ
    7. è®¿é—®è¯¦æƒ…é¡µ
    """

    print("=" * 70)
    print("å…¬å…±èµ„æºäº¤æ˜“ç½‘æ™ºèƒ½æœç´¢æµ‹è¯•")
    print("=" * 70)

    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    fetcher = PlaywrightFetcher()

    try:
        fetcher.start()

        # åˆ›å»ºæœç´¢å™¨
        searcher = CCGPSearcher(fetcher.page)

        # æ‰§è¡Œæœç´¢
        print("\nğŸ” æ‰§è¡Œæœç´¢:")
        print("  å…³é”®è¯: æ™ºèƒ½")
        print("  å“ç›®: å·¥ç¨‹ç±»")
        print("  æ—¶é—´: ä»Šæ—¥")
        print("  ç±»åˆ«: æ‰€æœ‰ç±»åˆ«")
        print("  ç±»å‹: æ‰€æœ‰ç±»å‹")

        results = searcher.search(
            keyword="æ™ºèƒ½",
            search_type="fulltext",  # æœå…¨æ–‡
            category="engineering",  # å·¥ç¨‹ç±»
            time_range="today",      # ä»Šæ—¥
            announcement_type="all",
            region="all",
            max_pages=2,  # æµ‹è¯•åªçˆ¬2é¡µ
        )

        print(f"\nâœ… æœç´¢å®Œæˆï¼è·å–åˆ° {len(results)} æ¡ç»“æœ")

        if not results:
            print("\nâš ï¸ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ç»“æœ")
            return

        # æ˜¾ç¤ºæœç´¢ç»“æœæ‘˜è¦
        print("\n" + "=" * 70)
        print("æœç´¢ç»“æœæ‘˜è¦:")
        print("=" * 70)

        for i, result in enumerate(results[:10], 1):
            print(f"\n{i}. {result.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
            print(f"   URL: {result.get('url', '')}")

        if len(results) > 10:
            print(f"\n... è¿˜æœ‰ {len(results) - 10} æ¡ç»“æœ")

        # ========== çˆ¬å–è¯¦æƒ…é¡µ ==========
        print("\n" + "=" * 70)
        print("å¼€å§‹çˆ¬å–è¯¦æƒ…é¡µ...")
        print("=" * 70)

        parser = CCGPAnnouncementParser()
        detailed_results = []

        # åªçˆ¬å‰3ä¸ªç»“æœä½œä¸ºæ¼”ç¤º
        for i, result in enumerate(results[:3], 1):
            url = result.get('url', '')
            if not url:
                continue

            print(f"\n[{i}/{len(results[:3])}] æ­£åœ¨çˆ¬å–: {result.get('title', '')[:40]}")
            print(f"     URL: {url}")

            try:
                # è·å–è¯¦æƒ…é¡µ
                html = fetcher.get_page(url)
                if not html:
                    print("     âŒ è·å–å¤±è´¥")
                    continue

                # è§£æè¯¦æƒ…é¡µ
                parsed = parser.parse(html, url)
                formatted = parser.format_for_storage(parsed)

                detailed_results.append(formatted)

                # æ˜¾ç¤ºå…³é”®ä¿¡æ¯
                print(f"     âœ… é¡¹ç›®åç§°: {formatted.get('project_name', '')[:40]}")
                print(f"     ğŸ“ é‡‡è´­äºº: {formatted.get('buyer_name', '')}")
                print(f"     ğŸ† ä¸­æ ‡äºº: {formatted.get('supplier', '')}")
                print(f"     ğŸ’° ä¸­æ ‡é‡‘é¢: {formatted.get('bid_amount', '')}")

            except Exception as e:
                print(f"     âŒ å¤±è´¥: {e}")

        # ========== ä¿å­˜ç»“æœ ==========
        print("\n" + "=" * 70)
        print("ä¿å­˜ç»“æœ...")
        print("=" * 70)

        import json
        from datetime import datetime

        output_file = Path("data/search_results.json")
        output_file.parent.mkdir(exist_ok=True)

        save_data = {
            'search_params': {
                'keyword': 'æ™ºèƒ½',
                'category': 'å·¥ç¨‹ç±»',
                'time_range': 'ä»Šæ—¥',
                'search_time': datetime.now().isoformat(),
            },
            'summary': {
                'total_results': len(results),
                'detailed_crawled': len(detailed_results),
            },
            'results': results,
            'detailed_results': detailed_results,
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)

        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

        # ========== ç»Ÿè®¡ä¿¡æ¯ ==========
        print("\n" + "=" * 70)
        print("ç»Ÿè®¡ä¿¡æ¯:")
        print("=" * 70)
        print(f"æœç´¢ç»“æœæ€»æ•°: {len(results)}")
        print(f"æˆåŠŸçˆ¬å–è¯¦æƒ…: {len(detailed_results)}")

        # ç»Ÿè®¡ä¸­æ ‡äºº
        suppliers = [r.get('supplier', '') for r in detailed_results if r.get('supplier')]
        if suppliers:
            print(f"\nä¸­æ ‡ä¼ä¸š:")
            for supplier in set(suppliers):
                count = suppliers.count(supplier)
                print(f"  - {supplier}: {count} ä¸ªé¡¹ç›®")

    finally:
        fetcher.stop()

    print("\n" + "=" * 70)
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print("=" * 70)


if __name__ == '__main__':
    test_intelligent_search()
