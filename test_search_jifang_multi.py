#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•ï¼šæœç´¢æœºæˆ¿ç›¸å…³å…¬å‘Š
ä½¿ç”¨å¤šä¸ªå…³é”®è¯å’Œåˆ†ç±»è¿›è¡Œæœç´¢
"""
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent))

from scraper.fetcher import PlaywrightFetcher
from scraper.ccgp_parser import CCGPAnnouncementParser
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import json
from datetime import datetime


def search_with_keywords():
    """
    ä½¿ç”¨å¤šä¸ªæœºæˆ¿ç›¸å…³å…³é”®è¯æœç´¢
    """

    # æœºæˆ¿ç›¸å…³å…³é”®è¯
    keywords = ["æœºæˆ¿", "æ™ºèƒ½åŒ–", "å¼±ç”µ", "ç½‘ç»œå·¥ç¨‹", "ä¿¡æ¯åŒ–", "æ•°æ®ä¸­å¿ƒ"]

    # å¤šä¸ªåˆ†ç±»ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
    categories = [
        {"name": "ä¸­å¤®å…¬å‘Š", "url": "https://www.ccgp.gov.cn/cggg/zygg/index.htm"},
        {"name": "ä¸­æ ‡å…¬å‘Š", "url": "https://www.ccgp.gov.cn/cggg/zybg/index.htm"},
        {"name": "å·¥ç¨‹ç±»", "url": "https://www.ccgp.gov.cn/cggg/gcgg/index.htm"},
    ]

    print("=" * 70)
    print("å…¬å…±èµ„æºäº¤æ˜“ç½‘ - æœºæˆ¿ç›¸å…³å…¬å‘Šæœç´¢")
    print("=" * 70)
    print(f"å…³é”®è¯: {', '.join(keywords)}")
    print(f"åˆ†ç±»: {[c['name'] for c in categories]}")

    fetcher = PlaywrightFetcher()
    all_results = []

    try:
        fetcher.start()

        for cat in categories:
            print(f"\n{'=' * 70}")
            print(f"æ­£åœ¨æœç´¢: {cat['name']}")
            print(f"{'=' * 70}")

            # çˆ¬å–åˆ—è¡¨é¡µ
            html = fetcher.get_page(cat['url'])
            if not html:
                print(f"âŒ è·å–é¡µé¢å¤±è´¥")
                continue

            soup = BeautifulSoup(html, 'lxml')

            # æŸ¥æ‰¾æ‰€æœ‰å…¬å‘Šé“¾æ¥
            found = []
            for link in soup.find_all('a'):
                href = link.get('href', '')
                title = link.get_text(strip=True)

                if href and 'htm' in href and len(title) > 10:
                    # æ„å»ºå®Œæ•´URL
                    if href.startswith('http'):
                        url = href
                    elif href.startswith('/'):
                        url = urljoin('https://www.ccgp.gov.cn/', href)
                    else:
                        continue

                    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»ä½•æœºæˆ¿ç›¸å…³å…³é”®è¯
                    if any(kw in title for kw in keywords):
                        found.append({
                            'title': title,
                            'url': url,
                            'category': cat['name'],
                            'matched_keywords': [kw for kw in keywords if kw in title],
                        })

            print(f"âœ… æ‰¾åˆ° {len(found)} æ¡ç›¸å…³å…¬å‘Š")

            all_results.extend(found)

            # æ˜¾ç¤ºå‰å‡ æ¡
            for i, item in enumerate(found[:5], 1):
                matched = ', '.join(item['matched_keywords'])
                print(f"  {i}. {item['title'][:60]}")
                print(f"     åŒ¹é…: {matched}")

    except Exception as e:
        print(f"âŒ æœç´¢è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

    finally:
        fetcher.stop()

    # ç»Ÿè®¡
    print(f"\n{'=' * 70}")
    print(f"æœç´¢å®Œæˆï¼")
    print(f"{'=' * 70}")
    print(f"æ€»å…±æ‰¾åˆ°: {len(all_results)} æ¡ç›¸å…³å…¬å‘Š")

    # æŒ‰å…³é”®è¯åˆ†ç±»ç»Ÿè®¡
    keyword_stats = {}
    for result in all_results:
        for kw in result.get('matched_keywords', []):
            keyword_stats[kw] = keyword_stats.get(kw, 0) + 1

    print(f"\nå…³é”®è¯åˆ†å¸ƒ:")
    for kw, count in sorted(keyword_stats.items(), key=lambda x: -x[1]):
        print(f"  {kw}: {count} æ¡")

    # çˆ¬å–è¯¦æƒ…é¡µï¼ˆå‰5æ¡ï¼‰
    if all_results:
        print(f"\n{'=' * 70}")
        print(f"çˆ¬å–è¯¦æƒ…é¡µ (å‰5æ¡)...")
        print(f"{'=' * 70}")

        parser = CCGPAnnouncementParser()
        detailed = []

        for i, result in enumerate(all_results[:5], 1):
            url = result['url']
            print(f"\n[{i}/5] {result['title'][:50]}")
            print(f"     åŒ¹é…: {', '.join(result['matched_keywords'])}")
            print(f"     URL: {url}")

            try:
                import time
                time.sleep(2)

                html = fetcher.get_page(url)
                if not html:
                    print("     âŒ è·å–å¤±è´¥")
                    continue

                parsed = parser.parse(html, url)
                formatted = parser.format_for_storage(parsed)
                detailed.append(formatted)

                print(f"     âœ… é¡¹ç›®: {formatted.get('project_name', '')[:40]}")
                print(f"     ğŸ† ä¸­æ ‡äºº: {formatted.get('supplier', '')[:30]}")

            except Exception as e:
                print(f"     âŒ å¤±è´¥: {e}")

        # ä¿å­˜ç»“æœ
        output_file = Path("data/jifang_related_results.json")
        output_file.parent.mkdir(exist_ok=True)

        save_data = {
            'search_time': datetime.now().isoformat(),
            'keywords': keywords,
            'total_found': len(all_results),
            'keyword_stats': keyword_stats,
            'results': all_results,
            'detailed_results': detailed,
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

        if detailed:
            print(f"\næˆåŠŸè§£æ {len(detailed)} æ¡è¯¦æƒ…")

            # æ‰¾å‡ºçœŸæ­£åŒ…å«"æœºæˆ¿"çš„é¡¹ç›®
            jifang_projects = [
                r for r in detailed
                if 'æœºæˆ¿' in r.get('project_name', '') or 'æœºæˆ¿' in r.get('title', '')
            ]

            if jifang_projects:
                print(f"\nâ­ åŒ…å«'æœºæˆ¿'çš„é¡¹ç›® ({len(jifang_projects)}æ¡):")
                for r in jifang_projects:
                    print(f"  - {r.get('project_name', '')[:60]}")
                    print(f"    ä¸­æ ‡äºº: {r.get('supplier', '')}")
    else:
        print("\nâš ï¸ æœªæ‰¾åˆ°ä»»ä½•ç›¸å…³å…¬å‘Š")


if __name__ == '__main__':
    search_with_keywords()
