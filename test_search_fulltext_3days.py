#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•ï¼šæœå…¨æ–‡ 3å¤©å†… - æœç´¢"æœºæˆ¿"
ä¸“é—¨æµ‹è¯•æ”¿åºœé‡‡è´­ç½‘æœç´¢å¹³å°çš„"æœå…¨æ–‡"åŠŸèƒ½
"""
import sys
from pathlib import Path
import json
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent))

from scraper.fetcher import PlaywrightFetcher
from scraper.ccgp_parser import CCGPAnnouncementParser


def test_search_fulltext_3days():
    """
    æµ‹è¯•æœå…¨æ–‡åŠŸèƒ½ - 3å¤©å†… - å…³é”®è¯"æœºæˆ¿"

    å°è¯•å¤šç§æ–¹æ³•æ¥è§¦å‘æœç´¢ï¼š
    1. ç›´æŽ¥ç‚¹å‡»"æœå…¨æ–‡"æŒ‰é’®
    2. æž„é€ æœç´¢APIè¯·æ±‚
    3. æ¨¡æ‹Ÿå®Œæ•´è¡¨å•å¡«å†™æµç¨‹
    """

    print("=" * 70)
    print("æ”¿åºœé‡‡è´­ç½‘æœç´¢å¹³å° - æœå…¨æ–‡æµ‹è¯•")
    print("=" * 70)

    print("\næµ‹è¯•å‚æ•°:")
    print("  å…³é”®è¯: æœºæˆ¿")
    print("  æœç´¢æ–¹å¼: æœå…¨æ–‡")
    print("  æ—¶é—´èŒƒå›´: 3å¤©å†…")
    print("  å“ç›®: å·¥ç¨‹ç±»")

    keyword = "æœºæˆ¿"

    # åˆ›å»ºçˆ¬è™«
    fetcher = PlaywrightFetcher()

    try:
        fetcher.start()
        page = fetcher.page

        # ========== æ–¹æ³•1: è®¿é—®æœç´¢å¹³å°å¹¶æ“ä½œ ==========
        print(f"\n{'=' * 70}")
        print("ã€æ–¹æ³•1ã€‘æœç´¢å¹³å°æ“ä½œæµ‹è¯•")
        print(f"{'=' * 70}")

        # å…ˆè®¿é—®ä¸»é¡µ
        print("\næ­¥éª¤1: è®¿é—®æ”¿åºœé‡‡è´­ç½‘ä¸»é¡µ...")
        try:
            page.goto("https://www.ccgp.gov.cn/",
                          wait_until="domcontentloaded",
                          timeout=30000)
            print("âœ… ä¸»é¡µè®¿é—®æˆåŠŸ")
            import time
            time.sleep(2)
        except Exception as e:
            print(f"âŒ ä¸»é¡µè®¿é—®å¤±è´¥: {e}")
            return

        # è®¿é—®æœç´¢å¹³å°
        print("\næ­¥éª¤2: è®¿é—®æœç´¢å¹³å°...")
        search_url = "https://search.ccgp.gov.cn/bxsearch"
        try:
            page.goto(search_url,
                          wait_until="domcontentloaded",
                          timeout=30000)
            print("âœ… æœç´¢å¹³å°åŠ è½½æˆåŠŸ")
            time.sleep(3)
        except Exception as e:
            print(f"âŒ æœç´¢å¹³å°è®¿é—®å¤±è´¥: {e}")
            return

        # æ£€æŸ¥æ˜¯å¦è¢«å°ç¦
        print("\næ­¥éª¤3: æ£€æŸ¥é¡µé¢çŠ¶æ€...")
        try:
            page_text = page.evaluate("() => document.body.innerText")
            if "è®¿é—®è¿‡äºŽé¢‘ç¹" in page_text or "ç¨åŽå†è¯•" in page_text:
                print("âŒ âš ï¸ æ£€æµ‹åˆ°åçˆ¬é™åˆ¶")
                print("\nå»ºè®®:")
                print("  1. ç­‰å¾…10-30åˆ†é’ŸåŽå†è¯•")
                print("  2. ä½¿ç”¨åˆ—è¡¨é¡µçˆ¬å–æ–¹å¼")
                return
            else:
                print("âœ… é¡µé¢æ­£å¸¸")
        except:
            pass

        # å°è¯•æŸ¥æ‰¾å’Œç‚¹å‡»æœç´¢ç›¸å…³å…ƒç´ 
        print("\næ­¥éª¤4: æŸ¥æ‰¾æœç´¢å…ƒç´ ...")

        # æŸ¥æ‰¾æœç´¢è¾“å…¥æ¡†
        input_found = False
        input_selectors = [
            '#kw',
            'input[name="kw"]',
            'input[placeholder*="è¯·è¾“å…¥"]',
            'input[type="text"]',
            '#keyword',
            'input[name="keyword"]',
            '.search-input',
        ]

        search_input = None
        for selector in input_selectors:
            try:
                elem = page.query_selector(selector)
                if elem:
                    search_input = elem
                    print(f"  âœ… æ‰¾åˆ°è¾“å…¥æ¡†: {selector}")
                    input_found = True
                    break
            except:
                continue

        if not search_input:
            print("  âŒ æœªæ‰¾åˆ°æœç´¢è¾“å…¥æ¡†")

        # æŸ¥æ‰¾"æœå…¨æ–‡"æŒ‰é’®
        button_found = False
        button_selectors = [
            '#doSearch2',
            'text="æœå…¨æ–‡"',
            'button:has-text("æœå…¨æ–‡")',
            '[onclick*="fulltext"]',
            '.btn-fulltext',
        ]

        fulltext_button = None
        for selector in button_selectors:
            try:
                elem = page.query_selector(selector)
                if elem:
                    fulltext_button = elem
                    print(f"  âœ… æ‰¾åˆ°æœå…¨æ–‡æŒ‰é’®: {selector}")
                    button_found = True
                    break
            except:
                continue

        if not button_found:
            print("  âŒ æœªæ‰¾åˆ°æœå…¨æ–‡æŒ‰é’®")

        # æŸ¥æ‰¾"3å¤©å†…"é€‰é¡¹
        time_3days_found = False
        time_selectors = [
            'text="è¿‘3æ—¥"',
            'text="3å¤©å†…"',
            '[value="3d"]',
        ]

        time_button = None
        for selector in time_selectors:
            try:
                elem = page.query_selector(selector)
                if elem:
                    time_button = elem
                    print(f"  âœ… æ‰¾åˆ°æ—¶é—´é€‰é¡¹: {selector}")
                    time_3days_found = True
                    break
            except:
                continue

        if not time_3days_found:
            print("  âš ï¸ æœªæ‰¾åˆ°'è¿‘3æ—¥'é€‰é¡¹")

        # å¦‚æžœæ‰¾åˆ°äº†å¿…è¦å…ƒç´ ï¼Œå°è¯•æ“ä½œ
        if input_found and button_found:
            print("\næ­¥éª¤5: æ‰§è¡Œæœç´¢æ“ä½œ...")

            # é‡æ–°èŽ·å–è¾“å…¥æ¡†å¹¶è¾“å…¥å…³é”®è¯
            try:
                search_input = page.query_selector('#kw') or page.query_selector('input[name="kw"]')
                if search_input:
                    search_input.fill(keyword)
                    print(f"  âœ… å·²è¾“å…¥å…³é”®è¯: {keyword}")
                    time.sleep(1)
            except Exception as e:
                print(f"  âŒ è¾“å…¥å…³é”®è¯å¤±è´¥: {e}")

            # é‡æ–°èŽ·å–å¹¶ç‚¹å‡»æ—¶é—´é€‰é¡¹
            if time_3days_found:
                try:
                    time_button = page.query_selector('text="è¿‘3æ—¥"')
                    if time_button:
                        time_button.click()
                        print(f"  âœ… å·²é€‰æ‹©: è¿‘3æ—¥")
                        time.sleep(1)
                except Exception as e:
                    print(f"  âš ï¸ é€‰æ‹©æ—¶é—´å¤±è´¥: {e}")

            # é‡æ–°èŽ·å–å¹¶ç‚¹å‡»æœå…¨æ–‡æŒ‰é’®
            try:
                fulltext_button = page.query_selector('#doSearch2')
                if fulltext_button:
                    fulltext_button.click()
                    print(f"  âœ… å·²ç‚¹å‡»: æœå…¨æ–‡")
                    time.sleep(5)  # ç­‰å¾…æœç´¢ç»“æžœ
            except Exception as e:
                print(f"  âŒ ç‚¹å‡»æœå…¨æ–‡å¤±è´¥: {e}")

            # å°è¯•èŽ·å–æœç´¢ç»“æžœ
            print("\næ­¥éª¤6: èŽ·å–æœç´¢ç»“æžœ...")

            # ç­‰å¾…ç»“æžœåŠ è½½
            time.sleep(3)

            # å…ˆæ‰“å°é¡µé¢å†…å®¹ç”¨äºŽè°ƒè¯•
            print("\n[è°ƒè¯•] æ£€æŸ¥é¡µé¢å†…å®¹...")
            try:
                page_text = page.evaluate("() => document.body.innerText")
                print(f"é¡µé¢æ–‡æœ¬é•¿åº¦: {len(page_text)} å­—ç¬¦")

                # æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹å®šå…³é”®è¯
                if "æ²¡æœ‰æ‰¾åˆ°" in page_text or "å…±0æ¡" in page_text or "æš‚æ— æ•°æ®" in page_text:
                    print("  âš ï¸ æœç´¢ç»“æžœä¸ºç©ºï¼ˆé¡µé¢æ˜¾ç¤ºæ— ç»“æžœï¼‰")
                elif "æ¡" in page_text and "æ‰¾åˆ°" in page_text:
                    # å°è¯•æå–ç»“æžœæ•°é‡
                    import re
                    match = re.search(r'å…±(\d+)æ¡|æ‰¾åˆ°.*?(\d+)æ¡', page_text)
                    if match:
                        count = match.group(1) or match.group(2)
                        print(f"  ðŸ“Š æœç´¢ç»“æžœæ•°é‡: {count} æ¡")

                # æ˜¾ç¤ºå‰500å­—ç¬¦
                if len(page_text) > 0:
                    print(f"\né¡µé¢å†…å®¹é¢„è§ˆï¼ˆå‰500å­—ç¬¦ï¼‰:")
                    print("-" * 70)
                    print(page_text[:500])
                    print("-" * 70)
            except Exception as e:
                print(f"  âŒ èŽ·å–é¡µé¢å†…å®¹å¤±è´¥: {e}")

            # èŽ·å–é¡µé¢å†…å®¹
            try:
                html = page.content()
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(html, 'lxml')

                # å°è¯•å¤šç§ç»“æžœé€‰æ‹©å™¨
                result_selectors = [
                    'li',  # å…ˆå°è¯•æ‰€æœ‰liå…ƒç´ 
                    'a',   # æˆ–è€…ç›´æŽ¥æ‰¾æ‰€æœ‰é“¾æŽ¥
                ]

                # ç›´æŽ¥ç”¨BeautifulSoupè§£æžï¼Œæ›´å¯é 
                results = []
                seen_urls = set()  # åŽ»é‡

                # æŸ¥æ‰¾æ‰€æœ‰é“¾æŽ¥
                for link in soup.find_all('a'):
                    try:
                        href = link.get('href', '')
                        title = link.get_text(strip=True)

                        # è¿‡æ»¤æœ‰æ•ˆçš„å…¬å‘Šé“¾æŽ¥
                        if (
                            href
                            and 'htm' in href
                            and len(title) > 10
                            and href not in seen_urls
                            # æŽ’é™¤å¯¼èˆªé“¾æŽ¥
                            and not any(x in href for x in ['index.htm', 'javascript', '#'])
                            # æŽ’é™¤å¯¼èˆªç±»æ ‡é¢˜
                            and not any(x in title for x in ['é¦–é¡µ', 'æ”¿é‡‡æ³•è§„', 'è´­ä¹°æœåŠ¡', 'ä¿¡æ¯å…¬å‘Š', 'æ‰€æœ‰ç±»åž‹', 'æ‰€æœ‰ç±»åˆ«', 'æ‰€æœ‰å“ç›®', 'ä»Šæ—¥', 'è¿‘3æ—¥', 'è¿‘1å‘¨'])
                        ):
                            # æž„å»ºå®Œæ•´URL
                            if not href.startswith('http'):
                                from urllib.parse import urljoin
                                url = urljoin('http://www.ccgp.gov.cn/', href)
                            else:
                                url = href

                            results.append({
                                'title': title,
                                'url': url,
                                'source': 'æœç´¢å¹³å°',
                            })
                            seen_urls.add(href)

                    except Exception:
                        continue

                print(f"  âœ… æå–åˆ° {len(results)} ä¸ªç»“æžœ")

                if results:
                    print(f"\næœç´¢ç»“æžœ (å‰5æ¡):")
                    print("-" * 70)
                    for i, r in enumerate(results[:5], 1):
                        print(f"{i}. {r['title'][:70]}")

                    # çˆ¬å–è¯¦æƒ…é¡µï¼ˆå‰2æ¡ï¼‰
                    print(f"\næ­¥éª¤7: çˆ¬å–è¯¦æƒ…é¡µ...")
                    parser = CCGPAnnouncementParser()
                    detailed = []

                    for r in results[:2]:
                        url = r['url']
                        if not url.startswith('http'):
                            from urllib.parse import urljoin
                            url = urljoin('http://www.ccgp.gov.cn/', url)

                        print(f"\n  çˆ¬å–: {r['title'][:40]}")

                        try:
                            time.sleep(3)
                            detail_html = fetcher.get_page(url)
                            if detail_html:
                                parsed = parser.parse(detail_html, url)
                                formatted = parser.format_for_storage(parsed)
                                detailed.append(formatted)

                                print(f"    âœ… é¡¹ç›®: {formatted.get('project_name', '')[:40]}")

                                # æ£€æŸ¥æ˜¯å¦åŒ…å«"æœºæˆ¿"
                                has_jifang = (
                                    'æœºæˆ¿' in formatted.get('project_name', '') or
                                    'æœºæˆ¿' in formatted.get('title', '')
                                )

                                if has_jifang:
                                    print(f"    â­ åŒ…å«'æœºæˆ¿'!")
                                else:
                                    print(f"    (æœªåŒ…å«'æœºæˆ¿'å…³é”®è¯)")
                        except Exception as e:
                            print(f"    âŒ å¤±è´¥: {e}")

                    # ä¿å­˜ç»“æžœ
                    output_file = Path("data/search_fulltext_3days_results.json")
                    output_file.parent.mkdir(exist_ok=True)

                    save_data = {
                        'search_params': {
                            'keyword': keyword,
                            'method': 'fulltext',
                            'time_range': '3days',
                            'search_time': datetime.now().isoformat(),
                        },
                        'results_count': len(results),
                        'results': results[:5],
                        'detailed_results': detailed,
                    }

                    with open(output_file, 'w', encoding='utf-8') as f:
                        json.dump(save_data, f, ensure_ascii=False, indent=2)

                    print(f"\nâœ… ç»“æžœå·²ä¿å­˜åˆ°: {output_file}")

                    if detailed:
                        print(f"\nç»Ÿè®¡:")
                        print(f"  æˆåŠŸè§£æž: {len(detailed)} æ¡")
                        jifang_count = sum(1 for r in detailed if 'æœºæˆ¿' in r.get('title', '') or 'æœºæˆ¿' in r.get('project_name', ''))
                        print(f"  åŒ…å«'æœºæˆ¿': {jifang_count} æ¡")

                else:
                    print("  âš ï¸ æœªæ‰¾åˆ°æœç´¢ç»“æžœå…ƒç´ ")

                    # æ˜¾ç¤ºé¡µé¢å†…å®¹ç”¨äºŽè°ƒè¯•
                    try:
                        page_text = page.evaluate("() => document.body.innerText")
                        if len(page_text) < 5000:
                            print("\n  é¡µé¢å†…å®¹:")
                            print("  " + page_text[:500])
                    except:
                        pass

            except Exception as e:
                print(f"  âŒ èŽ·å–æœç´¢ç»“æžœå¤±è´¥: {e}")

        else:
            print("\nâŒ ç¼ºå°‘å¿…è¦çš„æœç´¢å…ƒç´ ï¼Œæ— æ³•ç»§ç»­")
            print("\nðŸ’¡ å»ºè®®:")
            print("  1. æ£€æŸ¥ç½‘ç«™æ˜¯å¦æ›´æ–°äº†ç»“æž„")
            print("  2. å°è¯•ä½¿ç”¨åˆ—è¡¨é¡µçˆ¬å–æ–¹å¼")
            print("  3. æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•")

    finally:
        fetcher.stop()

    print("\n" + "=" * 70)
    print("âœ… æµ‹è¯•å®Œæˆ")
    print("=" * 70)


if __name__ == '__main__':
    test_search_fulltext_3days()
